"""
ğŸš€ Local LLM Integration System
Ollama + Llama 3.2 1B ë¡œì»¬ LLM í†µí•©
ë„¤íŠ¸ì›Œí¬ ì§€ì—° ì œê±° + ì´ˆê³ ì† ì¶”ë¡ 
"""
import asyncio
import httpx
import json
import logging
import time
import subprocess
import os
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class LocalModelType(Enum):
    LLAMA_32_1B = "llama3.2:1b"
    LLAMA_32_3B = "llama3.2:3b"
    PHI3_MINI = "phi3:mini"
    QWEN2_1B = "qwen2:1.5b"
    GEMMA2_2B = "gemma2:2b"

@dataclass
class LocalModelConfig:
    model_type: LocalModelType
    name: str
    max_tokens: int
    temperature: float = 0.0
    timeout: float = 1.0
    enabled: bool = True

class OllamaManager:
    """Ollama ë¡œì»¬ LLM ê´€ë¦¬ì"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.available_models = []
        self.model_configs = self._get_default_configs()
        self.is_running = False
    
    def _get_default_configs(self) -> List[LocalModelConfig]:
        """ê¸°ë³¸ ëª¨ë¸ ì„¤ì •"""
        return [
            LocalModelConfig(
                model_type=LocalModelType.LLAMA_32_1B,
                name="llama32_1b_ultra_fast",
                max_tokens=50,
                temperature=0.0,
                timeout=0.5
            ),
            LocalModelConfig(
                model_type=LocalModelType.PHI3_MINI,
                name="phi3_mini_balanced",
                max_tokens=80,
                temperature=0.0,
                timeout=1.0
            ),
            LocalModelConfig(
                model_type=LocalModelType.QWEN2_1B,
                name="qwen2_1b_fast",
                max_tokens=60,
                temperature=0.0,
                timeout=0.8
            )
        ]
    
    async def check_ollama_status(self) -> bool:
        """Ollama ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            async with httpx.AsyncClient(timeout=2.0) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                if response.status_code == 200:
                    self.is_running = True
                    data = response.json()
                    self.available_models = [model['name'] for model in data.get('models', [])]
                    logger.info(f"âœ… Ollama ì‹¤í–‰ ì¤‘, ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {len(self.available_models)}ê°œ")
                    return True
        except Exception as e:
            logger.warning(f"âŒ Ollama ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            self.is_running = False
        
        return False
    
    async def auto_install_models(self) -> bool:
        """í•„ìš”í•œ ëª¨ë¸ ìë™ ì„¤ì¹˜"""
        if not self.is_running:
            return False
        
        required_models = [config.model_type.value for config in self.model_configs]
        missing_models = [model for model in required_models if model not in self.available_models]
        
        if not missing_models:
            logger.info("âœ… ëª¨ë“  í•„ìš”í•œ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            return True
        
        logger.info(f"ğŸ“¥ {len(missing_models)}ê°œ ëª¨ë¸ ì„¤ì¹˜ ì‹œì‘: {missing_models}")
        
        for model in missing_models:
            try:
                logger.info(f"ğŸ“¥ {model} ì„¤ì¹˜ ì¤‘...")
                
                async with httpx.AsyncClient(timeout=300.0) as client:  # 5ë¶„ íƒ€ì„ì•„ì›ƒ
                    async with client.stream('POST', f"{self.base_url}/api/pull", 
                                           json={"name": model}) as response:
                        if response.status_code == 200:
                            async for chunk in response.aiter_text():
                                if chunk.strip():
                                    try:
                                        status = json.loads(chunk)
                                        if 'status' in status:
                                            logger.info(f"ğŸ“¥ {model}: {status['status']}")
                                    except:
                                        pass
                    
                    logger.info(f"âœ… {model} ì„¤ì¹˜ ì™„ë£Œ")
                    
            except Exception as e:
                logger.error(f"âŒ {model} ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
                return False
        
        # ì„¤ì¹˜ í›„ ëª¨ë¸ ëª©ë¡ ì—…ë°ì´íŠ¸
        await self.check_ollama_status()
        return True
    
    async def start_ollama_service(self) -> bool:
        """Ollama ì„œë¹„ìŠ¤ ì‹œì‘ (í•„ìš”ì‹œ)"""
        if await self.check_ollama_status():
            return True
        
        try:
            logger.info("ğŸš€ Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì‹œë„...")
            
            # macOS/Linuxì—ì„œ ollama serve ì‹¤í–‰
            process = subprocess.Popen(
                ['ollama', 'serve'],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                start_new_session=True
            )
            
            # ì„œë¹„ìŠ¤ ì‹œì‘ ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
            for i in range(10):
                await asyncio.sleep(1)
                if await self.check_ollama_status():
                    logger.info("âœ… Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì™„ë£Œ")
                    return True
            
            logger.warning("â° Ollama ì„œë¹„ìŠ¤ ì‹œì‘ íƒ€ì„ì•„ì›ƒ")
            return False
            
        except FileNotFoundError:
            logger.error("âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. https://ollama.ai/install ì—ì„œ ì„¤ì¹˜í•˜ì„¸ìš”")
            return False
        except Exception as e:
            logger.error(f"âŒ Ollama ì„œë¹„ìŠ¤ ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False

class LocalLLMProvider:
    """ë¡œì»¬ LLM ì œê³µì"""
    
    def __init__(self, config: LocalModelConfig, base_url: str = "http://localhost:11434"):
        self.config = config
        self.base_url = base_url
        self.model_name = config.model_type.value
        self.stats = {
            'total_calls': 0,
            'successful_calls': 0,
            'average_response_time': 0.0,
            'error_count': 0
        }
    
    async def generate(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        start_time = time.time()
        self.stats['total_calls'] += 1
        
        try:
            # í•œêµ­ì–´ ìµœì í™” í”„ë¡¬í”„íŠ¸
            optimized_prompt = self._optimize_prompt_for_korean(prompt)
            
            timeout = kwargs.get('timeout', self.config.timeout)
            max_tokens = kwargs.get('max_tokens', self.config.max_tokens)
            
            async with httpx.AsyncClient(timeout=timeout) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model_name,
                        "prompt": optimized_prompt,
                        "stream": False,
                        "options": {
                            "num_predict": max_tokens,
                            "temperature": self.config.temperature,
                            "top_k": 10,  # ë” ê²°ì •ì ì¸ ë‹µë³€
                            "top_p": 0.9,
                            "repeat_penalty": 1.1,
                            "stop": ["\\n\\n", "ì‚¬ìš©ì:", "User:", "ì§ˆë¬¸:"]  # ì¡°ê¸° ì¢…ë£Œ
                        }
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    generated_text = result.get('response', '').strip()
                    
                    # í›„ì²˜ë¦¬
                    processed_text = self._post_process_response(generated_text)
                    
                    processing_time = time.time() - start_time
                    self.stats['successful_calls'] += 1
                    self.stats['average_response_time'] = (
                        self.stats['average_response_time'] * (self.stats['successful_calls'] - 1) + processing_time
                    ) / self.stats['successful_calls']
                    
                    logger.info(f"ğŸš€ {self.config.name} ì‘ë‹µ: {processing_time:.3f}ì´ˆ")
                    
                    return {
                        'content': processed_text,
                        'model': self.config.name,
                        'processing_time': processing_time,
                        'success': True,
                        'tokens_generated': len(generated_text.split()),
                        'model_stats': result.get('eval_count', 0)
                    }
                else:
                    raise Exception(f"HTTP {response.status_code}: {response.text}")
                    
        except Exception as e:
            self.stats['error_count'] += 1
            processing_time = time.time() - start_time
            
            logger.error(f"âŒ {self.config.name} ì˜¤ë¥˜: {e}")
            
            return {
                'content': None,
                'model': self.config.name,
                'error': str(e),
                'success': False,
                'processing_time': processing_time
            }
    
    def _optimize_prompt_for_korean(self, prompt: str) -> str:
        """í•œêµ­ì–´ ì‘ë‹µì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        # ê°„ë‹¨í•˜ê³  ì§ì ‘ì ì¸ ì§€ì‹œ
        optimized = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ê°„ë‹¨í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. 1-2ë¬¸ì¥ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ì§ˆë¬¸: {prompt}

ë‹µë³€:"""
        
        return optimized
    
    def _post_process_response(self, text: str) -> str:
        """ì‘ë‹µ í›„ì²˜ë¦¬"""
        if not text:
            return "ë„¤, ì•Œê² ìŠµë‹ˆë‹¤!"
        
        # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
        text = text.strip()
        
        # ì˜ì–´ê°€ ì„ì—¬ìˆìœ¼ë©´ ì œê±° ì‹œë„
        lines = text.split('\n')
        korean_lines = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # í•œêµ­ì–´ ë¹„ìœ¨ ì²´í¬
            korean_chars = sum(1 for c in line if '\uac00' <= c <= '\ud7af')
            total_chars = len([c for c in line if c.isalpha()])
            
            if total_chars == 0 or korean_chars / total_chars > 0.3:
                korean_lines.append(line)
        
        result = ' '.join(korean_lines) if korean_lines else text
        
        # ìµœëŒ€ ê¸¸ì´ ì œí•œ
        if len(result) > 200:
            sentences = result.split('.')
            result = '.'.join(sentences[:2]) + ('.' if len(sentences) > 2 else '')
        
        return result if result else "ë„¤, ë§ì”€í•´ ì£¼ì„¸ìš”!"
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´"""
        total_calls = self.stats['total_calls']
        if total_calls == 0:
            return self.stats
        
        return {
            **self.stats,
            'success_rate': self.stats['successful_calls'] / total_calls * 100,
            'error_rate': self.stats['error_count'] / total_calls * 100
        }

class LocalLLMSystem:
    """ë¡œì»¬ LLM í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.ollama_manager = OllamaManager()
        self.providers = []
        self.is_initialized = False
        self.initialization_lock = asyncio.Lock()
    
    async def initialize(self) -> bool:
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        async with self.initialization_lock:
            if self.is_initialized:
                return True
            
            logger.info("ğŸš€ ë¡œì»¬ LLM ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹œì‘...")
            
            # 1. Ollama ìƒíƒœ í™•ì¸ ë° ì‹œì‘
            if not await self.ollama_manager.check_ollama_status():
                if not await self.ollama_manager.start_ollama_service():
                    logger.warning("âŒ Ollama ì„œë¹„ìŠ¤ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    return False
            
            # 2. í•„ìš”í•œ ëª¨ë¸ ì„¤ì¹˜
            if not await self.ollama_manager.auto_install_models():
                logger.warning("âš ï¸ ì¼ë¶€ ëª¨ë¸ ì„¤ì¹˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            
            # 3. ì œê³µì ì´ˆê¸°í™”
            self.providers = []
            for config in self.ollama_manager.model_configs:
                if config.model_type.value in self.ollama_manager.available_models:
                    provider = LocalLLMProvider(config)
                    self.providers.append(provider)
                    logger.info(f"âœ… {config.name} ì œê³µì ì´ˆê¸°í™” ì™„ë£Œ")
                else:
                    logger.warning(f"âš ï¸ {config.name} ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            if self.providers:
                self.is_initialized = True
                logger.info(f"ğŸ‰ ë¡œì»¬ LLM ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ: {len(self.providers)}ê°œ ì œê³µì")
                return True
            else:
                logger.error("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ë¡œì»¬ LLM ì œê³µìê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
    
    async def generate_fast(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """ê°€ì¥ ë¹ ë¥¸ ë¡œì»¬ LLMìœ¼ë¡œ ìƒì„±"""
        if not self.is_initialized:
            await self.initialize()
        
        if not self.providers:
            return {
                'content': None,
                'model': 'local_unavailable',
                'error': 'No local LLM providers available',
                'success': False,
                'processing_time': 0.0
            }
        
        # ê°€ì¥ ë¹ ë¥¸ ì œê³µì (ì²« ë²ˆì§¸) ì‚¬ìš©
        fastest_provider = self.providers[0]
        return await fastest_provider.generate(prompt, **kwargs)
    
    async def generate_parallel_local(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """ëª¨ë“  ë¡œì»¬ LLMì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ì—¬ ê°€ì¥ ë¹ ë¥¸ ì‘ë‹µ ì„ íƒ"""
        if not self.is_initialized:
            await self.initialize()
        
        if not self.providers:
            return await self.generate_fast(prompt, **kwargs)
        
        logger.info(f"ğŸš€ {len(self.providers)}ê°œ ë¡œì»¬ LLM ë³‘ë ¬ ì‹¤í–‰")
        
        # ëª¨ë“  ì œê³µì ë³‘ë ¬ ì‹¤í–‰
        tasks = []
        for provider in self.providers:
            task = asyncio.create_task(
                provider.generate(prompt, **kwargs),
                name=provider.config.name
            )
            tasks.append(task)
        
        try:
            # ì²« ë²ˆì§¸ ì„±ê³µí•œ ê²°ê³¼ ë°˜í™˜
            done, pending = await asyncio.wait(
                tasks,
                return_when=asyncio.FIRST_COMPLETED,
                timeout=2.0
            )
            
            # ë‚¨ì€ íƒœìŠ¤í¬ ì·¨ì†Œ
            for task in pending:
                task.cancel()
            
            # ì„±ê³µí•œ ê²°ê³¼ ì°¾ê¸°
            for task in done:
                try:
                    result = await task
                    if result['success'] and result.get('content'):
                        logger.info(f"ğŸ† ë¡œì»¬ LLM ìŠ¹ë¦¬: {result['model']}")
                        return result
                except Exception as e:
                    logger.error(f"ë¡œì»¬ LLM íƒœìŠ¤í¬ ì˜¤ë¥˜: {e}")
                    continue
            
            # ëª¨ë“  íƒœìŠ¤í¬ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
            return {
                'content': "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                'model': 'local_fallback',
                'success': True,
                'processing_time': 0.1,
                'is_fallback': True
            }
            
        except asyncio.TimeoutError:
            logger.warning("â° ë¡œì»¬ LLM ë³‘ë ¬ ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ")
            return await self.generate_fast(prompt, **kwargs)
    
    def get_system_stats(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„"""
        return {
            'is_initialized': self.is_initialized,
            'ollama_running': self.ollama_manager.is_running,
            'available_models': len(self.ollama_manager.available_models),
            'active_providers': len(self.providers),
            'provider_stats': {
                provider.config.name: provider.get_stats() 
                for provider in self.providers
            }
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ ì²´í¬"""
        if not self.is_initialized:
            await self.initialize()
        
        results = {
            'ollama_status': await self.ollama_manager.check_ollama_status(),
            'providers': {}
        }
        
        # ê° ì œê³µì í…ŒìŠ¤íŠ¸
        test_prompt = "ì•ˆë…•í•˜ì„¸ìš”"
        for provider in self.providers:
            try:
                start_time = time.time()
                result = await provider.generate(test_prompt, timeout=1.0, max_tokens=10)
                
                results['providers'][provider.config.name] = {
                    'status': 'healthy' if result['success'] else 'error',
                    'response_time': time.time() - start_time,
                    'error': result.get('error')
                }
                
            except Exception as e:
                results['providers'][provider.config.name] = {
                    'status': 'error',
                    'error': str(e)
                }
        
        return results

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
local_llm_system = LocalLLMSystem()