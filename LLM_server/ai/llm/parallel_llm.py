"""
ğŸš€ Parallel LLM System - Multi-Model First-Win Architecture
ì—¬ëŸ¬ LLMì„ ë™ì‹œì— í˜¸ì¶œí•˜ì—¬ ê°€ì¥ ë¹ ë¥¸ ì‘ë‹µ ì„ íƒ
"""
import asyncio
import httpx
import time
import logging
import json
import os
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass
from enum import Enum
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

logger = logging.getLogger(__name__)

class ModelPriority(Enum):
    ULTRA_FAST = 1
    BALANCED = 2  
    ACCURATE = 3
    LOCAL = 4

@dataclass
class ModelConfig:
    name: str
    model_id: str
    max_tokens: int
    timeout: float
    priority: ModelPriority
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    enabled: bool = True

class LLMProvider:
    """ê°œë³„ LLM ì œê³µì ì¶”ìƒ í´ë˜ìŠ¤"""
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.client = None
        self._initialize_client()
    
    def _initialize_client(self):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        pass
    
    async def generate(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ìƒì„±"""
        raise NotImplementedError

class OpenAIProvider(LLMProvider):
    """OpenAI GPT ì œê³µì"""
    
    def _initialize_client(self):
        self.client = ChatOpenAI(
            model=self.config.model_id,
            api_key=self.config.api_key or os.getenv("OPENAI_API_KEY"),
            max_tokens=self.config.max_tokens,
            temperature=0,
            timeout=self.config.timeout,
            max_retries=0,
            streaming=False
        )
    
    async def generate(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        try:
            start_time = time.time()
            
            # ë©”ì‹œì§€ ë³€í™˜
            langchain_messages = []
            for msg in messages:
                if msg["role"] == "system":
                    langchain_messages.append(SystemMessage(content=msg["content"]))
                else:
                    langchain_messages.append(HumanMessage(content=msg["content"]))
            
            response = await self.client.ainvoke(langchain_messages)
            
            return {
                'content': response.content,
                'model': self.config.name,
                'processing_time': time.time() - start_time,
                'success': True,
                'priority': self.config.priority.value
            }
            
        except Exception as e:
            logger.error(f"{self.config.name} ì˜¤ë¥˜: {e}")
            return {
                'content': None,
                'model': self.config.name,
                'error': str(e),
                'success': False,
                'processing_time': time.time() - start_time if 'start_time' in locals() else 0
            }

class OllamaProvider(LLMProvider):
    """Ollama ë¡œì»¬ LLM ì œê³µì"""
    
    async def generate(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        try:
            start_time = time.time()
            
            # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ë§Œ ì‚¬ìš© (ì†ë„ ìµœì í™”)
            user_message = ""
            for msg in reversed(messages):
                if msg["role"] == "user":
                    user_message = msg["content"]
                    break
            
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.post(
                    f"{self.config.base_url}/api/generate",
                    json={
                        "model": self.config.model_id,
                        "prompt": user_message,
                        "stream": False,
                        "options": {
                            "num_predict": self.config.max_tokens,
                            "temperature": 0
                        }
                    }
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return {
                        'content': result.get('response', ''),
                        'model': self.config.name,
                        'processing_time': time.time() - start_time,
                        'success': True,
                        'priority': self.config.priority.value
                    }
                else:
                    raise Exception(f"HTTP {response.status_code}: {response.text}")
                    
        except Exception as e:
            logger.warning(f"{self.config.name} ì˜¤ë¥˜: {e}")
            return {
                'content': None,
                'model': self.config.name,
                'error': str(e),
                'success': False,
                'processing_time': time.time() - start_time if 'start_time' in locals() else 0
            }

class ClaudeProvider(LLMProvider):
    """Anthropic Claude ì œê³µì (í–¥í›„ í™•ì¥ìš©)"""
    
    async def generate(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        # Claude API êµ¬í˜„ (í˜„ì¬ëŠ” ìŠ¤í‚µ)
        return {
            'content': None,
            'model': self.config.name,
            'error': 'Claude provider not implemented',
            'success': False,
            'processing_time': 0
        }

class ParallelLLMSystem:
    """ë³‘ë ¬ LLM í˜¸ì¶œ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.providers = []
        self.stats = {
            'total_calls': 0,
            'successful_calls': 0,
            'model_wins': {},
            'average_response_time': 0.0
        }
        self._initialize_providers()
    
    def _initialize_providers(self):
        """LLM ì œê³µìë“¤ ì´ˆê¸°í™”"""
        
        # GPT-3.5-turbo (ì´ˆê³ ì†)
        gpt35_config = ModelConfig(
            name="gpt35_turbo",
            model_id="gpt-3.5-turbo",
            max_tokens=80,
            timeout=2.0,
            priority=ModelPriority.ULTRA_FAST,
            enabled=bool(os.getenv("OPENAI_API_KEY"))
        )
        if gpt35_config.enabled:
            self.providers.append(OpenAIProvider(gpt35_config))
        
        # GPT-4o-mini (ê· í˜•)
        gpt4mini_config = ModelConfig(
            name="gpt4o_mini",
            model_id="gpt-4o-mini",
            max_tokens=120,
            timeout=3.0,
            priority=ModelPriority.BALANCED,
            enabled=bool(os.getenv("OPENAI_API_KEY"))
        )
        if gpt4mini_config.enabled:
            self.providers.append(OpenAIProvider(gpt4mini_config))
        
        # Ollama Llama 3.2 (ë¡œì»¬)
        ollama_config = ModelConfig(
            name="llama32_local",
            model_id="llama3.2:1b",
            max_tokens=60,
            timeout=1.0,
            priority=ModelPriority.LOCAL,
            base_url="http://localhost:11434",
            enabled=True  # í•­ìƒ ì‹œë„ (ì‹¤íŒ¨í•´ë„ OK)
        )
        self.providers.append(OllamaProvider(ollama_config))
        
        logger.info(f"ğŸš€ {len(self.providers)}ê°œ LLM ì œê³µì ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def generate_parallel(self, messages: List[Dict[str, str]], 
                              strategy: str = "first_win",
                              quality_check: bool = True) -> Dict[str, Any]:
        """ë³‘ë ¬ LLM í˜¸ì¶œ"""
        
        if not self.providers:
            return {
                'content': "ì£„ì†¡í•©ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ LLMì´ ì—†ìŠµë‹ˆë‹¤.",
                'model': 'fallback',
                'success': False,
                'error': 'No providers available'
            }
        
        start_time = time.time()
        self.stats['total_calls'] += 1
        
        # í™œì„±í™”ëœ ì œê³µìë“¤ë¡œ ë³‘ë ¬ í˜¸ì¶œ
        tasks = []
        for provider in self.providers:
            if provider.config.enabled:
                task = asyncio.create_task(
                    provider.generate(messages),
                    name=provider.config.name
                )
                tasks.append((task, provider))
        
        if not tasks:
            return {
                'content': "ì£„ì†¡í•©ë‹ˆë‹¤. í™œì„±í™”ëœ LLMì´ ì—†ìŠµë‹ˆë‹¤.",
                'model': 'fallback',
                'success': False
            }
        
        logger.info(f"ğŸš€ {len(tasks)}ê°œ LLM ë³‘ë ¬ í˜¸ì¶œ ì‹œì‘")
        
        if strategy == "first_win":
            return await self._first_win_strategy(tasks, start_time, quality_check)
        elif strategy == "race_with_timeout":
            return await self._race_with_timeout_strategy(tasks, start_time, 1.0)
        else:
            return await self._best_quality_strategy(tasks, start_time)
    
    async def _first_win_strategy(self, tasks: List[Tuple], start_time: float, 
                                quality_check: bool) -> Dict[str, Any]:
        """ì²« ë²ˆì§¸ ì„±ê³µí•œ ì‘ë‹µ ì„ íƒ"""
        
        completed_tasks = []
        pending_tasks = [task for task, _ in tasks]
        
        try:
            while pending_tasks:
                done, pending_tasks = await asyncio.wait(
                    pending_tasks, 
                    return_when=asyncio.FIRST_COMPLETED,
                    timeout=0.5
                )
                
                for task in done:
                    result = await task
                    completed_tasks.append(result)
                    
                    if result['success'] and result.get('content'):
                        # í’ˆì§ˆ ì²´í¬
                        if not quality_check or self._is_quality_response(result['content']):
                            # ìŠ¹ì ê¸°ë¡
                            model_name = result['model']
                            self.stats['model_wins'][model_name] = self.stats['model_wins'].get(model_name, 0) + 1
                            self.stats['successful_calls'] += 1
                            
                            # ë‚¨ì€ íƒœìŠ¤í¬ë“¤ ì·¨ì†Œ
                            for pending_task in pending_tasks:
                                pending_task.cancel()
                            
                            total_time = time.time() - start_time
                            logger.info(f"ğŸ† {model_name} ìŠ¹ë¦¬! ì´ ì‹œê°„: {total_time:.3f}ì´ˆ")
                            
                            return {
                                **result,
                                'total_processing_time': total_time,
                                'completed_models': len(completed_tasks)
                            }
                
                # íƒ€ì„ì•„ì›ƒ ì²´í¬
                if time.time() - start_time > 3.0:
                    logger.warning("â° ëª¨ë“  LLM íƒ€ì„ì•„ì›ƒ")
                    break
            
            # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ í›„ì—ë„ ì„±ê³µí•œ ì‘ë‹µì´ ì—†ëŠ” ê²½ìš°
            successful_results = [r for r in completed_tasks if r['success']]
            if successful_results:
                # ìš°ì„ ìˆœìœ„ ê¸°ì¤€ìœ¼ë¡œ ì„ íƒ
                best_result = min(successful_results, key=lambda x: x['priority'])
                return {
                    **best_result,
                    'total_processing_time': time.time() - start_time,
                    'completed_models': len(completed_tasks)
                }
            
        except Exception as e:
            logger.error(f"ë³‘ë ¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        # ëª¨ë“  ì‹¤íŒ¨ ì‹œ í´ë°±
        return await self._get_fallback_response(start_time)
    
    async def _race_with_timeout_strategy(self, tasks: List[Tuple], start_time: float, 
                                        timeout: float) -> Dict[str, Any]:
        """íƒ€ì„ì•„ì›ƒ ê¸°ë°˜ ë ˆì´ìŠ¤"""
        try:
            done, pending = await asyncio.wait(
                [task for task, _ in tasks],
                timeout=timeout,
                return_when=asyncio.FIRST_COMPLETED
            )
            
            # ë‚¨ì€ íƒœìŠ¤í¬ ì·¨ì†Œ
            for task in pending:
                task.cancel()
            
            # ì™„ë£Œëœ íƒœìŠ¤í¬ ì¤‘ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ ì„ íƒ
            results = []
            for task in done:
                try:
                    result = await task
                    if result['success']:
                        results.append(result)
                except:
                    pass
            
            if results:
                best_result = min(results, key=lambda x: x['priority'])
                return {
                    **best_result,
                    'total_processing_time': time.time() - start_time
                }
            
        except asyncio.TimeoutError:
            logger.warning(f"â° {timeout}ì´ˆ íƒ€ì„ì•„ì›ƒ")
        
        return await self._get_fallback_response(start_time)
    
    def _is_quality_response(self, content: str) -> bool:
        """ì‘ë‹µ í’ˆì§ˆ ì²´í¬"""
        if not content or len(content.strip()) < 2:
            return False
        
        # ì—ëŸ¬ ë©”ì‹œì§€ íŒ¨í„´ ì²´í¬
        error_patterns = ['error', 'ì˜¤ë¥˜', 'sorry', 'ì£„ì†¡', 'cannot', 'í•  ìˆ˜ ì—†']
        content_lower = content.lower()
        
        # ì—ëŸ¬ íŒ¨í„´ì´ ë„ˆë¬´ ë§ìœ¼ë©´ í’ˆì§ˆì´ ë‚®ìŒ
        error_count = sum(1 for pattern in error_patterns if pattern in content_lower)
        
        return error_count <= 1 and len(content.strip()) >= 10
    
    async def _get_fallback_response(self, start_time: float) -> Dict[str, Any]:
        """í´ë°± ì‘ë‹µ"""
        fallback_responses = [
            "ë„¤, ë§ì”€í•´ ì£¼ì„¸ìš”!",
            "ì•Œê² ìŠµë‹ˆë‹¤. ë” ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.",
            "ë„¤, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
            "ì¢‹ìŠµë‹ˆë‹¤!"
        ]
        
        import random
        response = random.choice(fallback_responses)
        
        return {
            'content': response,
            'model': 'fallback',
            'success': True,
            'total_processing_time': time.time() - start_time,
            'is_fallback': True
        }
    
    def get_stats(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„"""
        total_calls = self.stats['total_calls']
        if total_calls == 0:
            return self.stats
        
        return {
            **self.stats,
            'success_rate': self.stats['successful_calls'] / total_calls * 100,
            'provider_count': len(self.providers),
            'enabled_providers': len([p for p in self.providers if p.config.enabled])
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬"""
        results = {}
        
        test_messages = [{"role": "user", "content": "ì•ˆë…•í•˜ì„¸ìš”"}]
        
        for provider in self.providers:
            if not provider.config.enabled:
                results[provider.config.name] = {'status': 'disabled'}
                continue
                
            try:
                start_time = time.time()
                result = await provider.generate(test_messages)
                
                results[provider.config.name] = {
                    'status': 'healthy' if result['success'] else 'error',
                    'response_time': time.time() - start_time,
                    'error': result.get('error')
                }
                
            except Exception as e:
                results[provider.config.name] = {
                    'status': 'error',
                    'error': str(e)
                }
        
        return results

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
parallel_llm = ParallelLLMSystem()