

import json
import os
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from dotenv import load_dotenv
import time
import logging
from ..prompts import prompt

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class JSONToRAGWithHistory:
    def __init__(self, json_file_path: str, openai_api_key: str = None):
        self.json_file_path = json_file_path
        self.openai_api_key = openai_api_key or os.getenv("OPENAI_API_KEY")
        
        if not self.openai_api_key:
            raise ValueError("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        
        self.vectorstore = None
        self.rag_chain = None
        self.conversational_rag_chain = None
        self.store = {}  # ì„¸ì…˜ë³„ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ì €ì¥ì†Œ
        
    def load_json_data(self):
        """JSON íŒŒì¼ ë¡œë“œ"""
        try:
            with open(self.json_file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            print(f"âœ… JSON ë¡œë“œ ì™„ë£Œ: {len(data['conversations'])}ê°œ ëŒ€í™”")
            return data
        except FileNotFoundError:
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.json_file_path}")
            return None
        except json.JSONDecodeError:
            print(f"âŒ JSON íŒŒì‹± ì—ëŸ¬: {self.json_file_path}")
            return None
    
    def create_documents(self, data):
        """JSONì„ LangChain Documentë¡œ ë³€í™˜"""
        documents = []
        
        for conv in data["conversations"]:
            # Q&A í˜•íƒœë¡œ content êµ¬ì„±
            content = f"""ì‚¬ìš©ì ì§ˆë¬¸: {conv['user_question']}
                        AI ë‹µë³€: {conv['ai_answer']}
                        ë‚ ì§œ: {conv['date']}
                        ì‹œê°„: {conv['time']}"""
                                    
            # Document ìƒì„±
            doc = Document(
                page_content=content,
                metadata={
                    "conversation_id": conv["id"],
                    "timestamp": conv["timestamp"],
                    "date": conv["date"],
                    "time": conv["time"],
                    "user_id": data["user_id"],
                    "user_question": conv["user_question"],
                    "ai_answer": conv["ai_answer"]
                }
            )
            documents.append(doc)
        
        print(f"âœ… Document ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ")
        return documents
    
    # create_vectorstoreë„ ë””ë²„ê¹… ê°•í™”
    def create_vectorstore(self, documents):
        """ë²¡í„° ìŠ¤í† ì–´ ìƒì„± (ë””ë²„ê¹… ê°•í™”)"""
        print("ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹œì‘...")
        print(f"ğŸ“Š ì…ë ¥ ë¬¸ì„œ ê°œìˆ˜: {len(documents)}")
        
        if not documents:
            print("âš ï¸ ë¬¸ì„œê°€ ì—†ìŒ - ë”ë¯¸ ë¬¸ì„œ ìƒì„±")
            # ë”ë¯¸ ë¬¸ì„œ ìƒì„±
            dummy_doc = Document(
                page_content="ì´ˆê¸° ì„¤ì •ìš© ë”ë¯¸ ë¬¸ì„œì…ë‹ˆë‹¤. ì²« ëŒ€í™” í›„ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.",
                metadata={
                    "conversation_id": "dummy",
                    "timestamp": "2024-01-01T00:00:00",
                    "date": "2024-01-01",
                    "time": "00:00:00",
                    "user_id": "system",
                    "user_question": "ì´ˆê¸° ì„¤ì •",
                    "ai_answer": "ë”ë¯¸ ë°ì´í„°"
                }
            )
            documents = [dummy_doc]
            print(f"âœ… ë”ë¯¸ ë¬¸ì„œ ìƒì„±: {len(documents)}ê°œ")
        
        try:
            print("ğŸ”„ í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
            # í…ìŠ¤íŠ¸ ë¶„í• 
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=300,
                chunk_overlap=50,
                separators=["\n\n", "\n", " ", ""]
            )
            
            split_docs = text_splitter.split_documents(documents)
            print(f"âœ… í…ìŠ¤íŠ¸ ë¶„í•  ì™„ë£Œ: {len(split_docs)}ê°œ ì²­í¬")
            
            if split_docs:
                print(f"ğŸ“„ ì²« ë²ˆì§¸ ì²­í¬: {split_docs[0].page_content[:100]}...")
            
            print("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
            # ì„ë² ë”© ìƒì„±
            embeddings = OpenAIEmbeddings(
                openai_api_key=self.openai_api_key,
                model="text-embedding-3-small"
            )
            print("âœ… ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
            print("ğŸ”„ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘...")
            # FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
            vectorstore = FAISS.from_documents(split_docs, embeddings)
            print("âœ… FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
            
            # ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´ ì •ë³´ í™•ì¸
            if hasattr(vectorstore, 'index'):
                print(f"ğŸ“Š ìƒì„±ëœ ë²¡í„° ê°œìˆ˜: {vectorstore.index.ntotal}")
                print(f"ğŸ“Š ë²¡í„° ì°¨ì›: {vectorstore.index.d}")
            
            # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰
            print("ğŸ” í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ìˆ˜í–‰...")
            test_results = vectorstore.similarity_search("ì•ˆë…•", k=1)
            print(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ ê²°ê³¼: {len(test_results)}ê°œ")
            if test_results:
                print(f"ğŸ“„ ê²€ìƒ‰ëœ ë¬¸ì„œ: {test_results[0].page_content[:100]}...")
            
            return vectorstore
            
        except Exception as e:
            print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return None

    
    def format_docs(self, docs):
        """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í¬ë§·íŒ…"""
        return "\n\n".join(doc.page_content for doc in docs)
    
    def create_rag_chain(self, vectorstore):
        """RAG ì²´ì¸ ìƒì„± (RunnableWithMessageHistory ì‚¬ìš©)"""
        print("ğŸ”„ RAG ì²´ì¸ êµ¬ì„± ì¤‘...")
        
        # LLM ì„¤ì •
        llm = ChatOpenAI(
            openai_api_key=self.openai_api_key,
            model="gpt-4o-mini",
            temperature=0.1,
            base_url="https://api.openai.com/v1",
            timeout=30,
            max_retries=2
        )
        
        # ë¦¬íŠ¸ë¦¬ë²„ 
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": 5}  # kë¥¼ ëŠ˜ë ¤ì„œ ë” ë§ì´ ê²€ìƒ‰
        )
        
        # âœ… ìˆ˜ì •: ë””ë²„ê¹…ì´ ê°•í™”ëœ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ í•¨ìˆ˜
        def get_context(inputs):
            """ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ í•¨ìˆ˜ - ê°•í™”ëœ ë””ë²„ê¹…"""
            question = inputs["input"]
            print(f"\nğŸ” === ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹œì‘ ===")
            print(f"ğŸ“ ê²€ìƒ‰ ì§ˆë¬¸: '{question}'")
            
            try:
                # 1. ê¸°ë³¸ ê²€ìƒ‰
                docs = retriever.invoke(question)
                print(f"ğŸ“Š ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
                
                # 2. ê²€ìƒ‰ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
                if docs:
                    print(f"âœ… ê²€ìƒ‰ ì„±ê³µ! ë¬¸ì„œ ì •ë³´:")
                    for i, doc in enumerate(docs):
                        metadata = doc.metadata
                        conv_id = metadata.get('conversation_id', 'N/A')
                        user_q = metadata.get('user_question', 'N/A')[:50]
                     
                    
                    context = self.format_docs(docs)
                    return context
                else:
                    print("âŒ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŒ!")
                    
                    # 3. ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ì¬ê²€ìƒ‰
                    print("ğŸ” ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨ ì¬ê²€ìƒ‰...")
                    try:
                        docs_with_scores = vectorstore.similarity_search_with_score(question, k=5)
                        print(f"ğŸ“Š ì ìˆ˜ í¬í•¨ ê²€ìƒ‰ ê²°ê³¼: {len(docs_with_scores)}ê°œ")
                        
                        if docs_with_scores:
                            print("ğŸ“‹ ì ìˆ˜ë³„ ê²€ìƒ‰ ê²°ê³¼:")
                            for i, (doc, score) in enumerate(docs_with_scores):
                                metadata = doc.metadata
                                conv_id = metadata.get('conversation_id', 'N/A')
                                user_q = metadata.get('user_question', 'N/A')[:30]
                                print(f"  {i+1}. [ëŒ€í™”#{conv_id}] ì ìˆ˜:{score:.3f} {user_q}...")
                            
                            # ì ìˆ˜ê°€ ë†’ì€ ë¬¸ì„œë“¤ ë°˜í™˜ (ì ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ìœ ì‚¬í•¨)
                            best_docs = [doc for doc, score in docs_with_scores if score < 1.0]
                            if best_docs:
                                context = self.format_docs(best_docs)
                                print(f"âœ… ìœ ì‚¬ë„ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ìƒì„±: {len(context)}ì")
                                return context
                    except Exception as score_error:
                        print(f"âŒ ì ìˆ˜ ê²€ìƒ‰ ì˜¤ë¥˜: {score_error}")
                    
                    # 4. ë²¡í„°ìŠ¤í† ì–´ ìƒíƒœ í™•ì¸
                    print("ğŸ” ë²¡í„°ìŠ¤í† ì–´ ìƒíƒœ í™•ì¸...")
                    if hasattr(vectorstore, 'index'):
                        print(f"ğŸ“Š ì´ ë²¡í„° ìˆ˜: {vectorstore.index.ntotal}")
                        print(f"ğŸ“Š ë²¡í„° ì°¨ì›: {vectorstore.index.d}")
                    
                    # 5. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì‹œë„
                    print("ğŸ” ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ì‹œë„...")
                    test_keywords = ["ì•ˆë…•", "ì´ë¦„", "ì§ˆë¬¸", "ëŒ€í™”", question.split()[0] if question.split() else ""]
                    for keyword in test_keywords:
                        if keyword:
                            try:
                                test_docs = vectorstore.similarity_search(keyword, k=2)
                                if test_docs:
                                    print(f"  '{keyword}' ê²€ìƒ‰: {len(test_docs)}ê°œ ë¬¸ì„œ ë°œê²¬")
                                    context = self.format_docs(test_docs)
                                    print(f"âœ… ëŒ€ì²´ í‚¤ì›Œë“œë¡œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±")
                                    return context
                            except:
                                continue
                    
                    print("âŒ ëª¨ë“  ê²€ìƒ‰ ì‹œë„ ì‹¤íŒ¨")
                    return "ê²€ìƒ‰ëœ ì´ì „ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤."
                    
            except Exception as e:
                print(f"âŒ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
                return "ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        # RAG ì²´ì¸ êµ¬ì„±
        rag_chain = (
            {
                "context": get_context,
                "input": lambda x: x["input"],
                "chat_history": lambda x: x.get("chat_history", [])
            }
            | prompt
            | llm
            | StrOutputParser()
        )
        
        print("âœ… RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
        return rag_chain
    
    def get_session_history(self, session_id: str):
        """ì„¸ì…˜ë³„ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°"""
        if session_id not in self.store:
            self.store[session_id] = ChatMessageHistory()
        return self.store[session_id]
    
    def create_conversational_rag_chain(self, rag_chain):
        """ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„±"""
        print("ğŸ”„ ëŒ€í™”í˜• RAG ì²´ì¸ êµ¬ì„± ì¤‘...")
        
        # RunnableWithMessageHistoryë¡œ ë˜í•‘
        conversational_rag_chain = RunnableWithMessageHistory(
            rag_chain,
            self.get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
        )
        
        print("âœ… ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„± ì™„ë£Œ")
        return conversational_rag_chain
    
    
    
    
    def build_rag_system(self):
        """ì „ì²´ RAG ì‹œìŠ¤í…œ êµ¬ì¶•"""
        print("ğŸš€ RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì‹œì‘...")
        
        # 1. JSON ë°ì´í„° ë¡œë“œ
        data = self.load_json_data()
        print(data)
       
        if not data:
            return False
        
        # 2. Document ìƒì„±
        documents = self.create_documents(data)
        
        # 3. ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        self.vectorstore = self.create_vectorstore(documents)
        
        # 4. RAG ì²´ì¸ ìƒì„±
        self.rag_chain = self.create_rag_chain(self.vectorstore)
        
        # 5. ëŒ€í™”í˜• RAG ì²´ì¸ ìƒì„±
        self.conversational_rag_chain = self.create_conversational_rag_chain(self.rag_chain)
        
        print("ğŸ‰ RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!")
        return True
    
    def stream_query(self, question: str, session_id: str = "default", cancel_event=None, is_eos_received_func=None):
        """ìŠ¤íŠ¸ë¦¼ ì§ˆë¬¸í•˜ê¸° (ì¤‘ë‹¨ ì´ë²¤íŠ¸ ì§€ì›)"""
        try:
            config = {"configurable": {"session_id": session_id}}
            
            print(f"ğŸ” ìŠ¤íŠ¸ë¦¼ ì§ˆì˜ ì‹œì‘: {question}")
            
            chunk_count = 0
            for chunk in self.conversational_rag_chain.stream({"input": question}, config=config):
                chunk_count += 1
                
                # ì¤‘ë‹¨ ì´ë²¤íŠ¸ ì²´í¬
                if cancel_event and is_eos_received_func:
                    if cancel_event.is_set() and not is_eos_received_func():
                        print(f"âš ï¸ ìŠ¤íŠ¸ë¦¼ ì¤‘ë‹¨ ê°ì§€ (ì²­í¬ #{chunk_count})")
                        break
                
                if chunk:
                    yield chunk
            
        except Exception as e:
            print(f"âŒ ìŠ¤íŠ¸ë¦¼ ì§ˆì˜ ì˜¤ë¥˜: {e}")
            yield f"ì˜¤ë¥˜: {str(e)}"
    
    def query(self, question: str, session_id: str = "default", show_sources: bool = True):
        """ì§ˆë¬¸í•˜ê¸° (ëŒ€í™” ê¸°ë¡ í¬í•¨)"""
        
        print(f"\nğŸ¤” ì§ˆë¬¸: {question}")
        print("ğŸ”„ ê²€ìƒ‰ ì¤‘...")
        
        try:
            # ì§ˆì˜ ì‹¤í–‰ (ì„¸ì…˜ ID í¬í•¨)
            config = {"configurable": {"session_id": session_id}}
            result = self.conversational_rag_chain.stream(
                {"input": question},
                config=config
            )
            
            print(f"ğŸ¤– ë‹µë³€: {result}")
            
            if show_sources:
                # ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰í•´ì„œ ë³´ì—¬ì£¼ê¸°
                retriever = self.vectorstore.as_retriever(
                    search_type="similarity",
                    search_kwargs={"k": 3}
                )
                docs = retriever.invoke(question)
                
                if docs:
                    print(f"\nğŸ“š ì°¸ê³ í•œ ëŒ€í™” ({len(docs)}ê°œ):")
                    for i, doc in enumerate(docs, 1):
                        metadata = doc.metadata
                        print(f"  {i}. [ëŒ€í™” #{metadata['conversation_id']}] {metadata['date']}")
                        print(f"     ğŸ‘¤ {metadata['user_question'][:50]}...")
                        print(f"     ğŸ¤– {metadata['ai_answer'][:50]}...")
                        print()
            
            return result
            
        except Exception as e:
            print(f"âŒ ì§ˆì˜ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return None
    
    def get_chat_history(self, session_id: str = "default"):
        """í˜„ì¬ ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ ì¡°íšŒ"""
        if session_id in self.store:
            history = self.store[session_id]
            messages = history.messages
            print(f"\nğŸ’¬ ì„¸ì…˜ '{session_id}'ì˜ ëŒ€í™” ê¸°ë¡:")
            for i, msg in enumerate(messages, 1):
                role = "ğŸ‘¤ ì‚¬ìš©ì" if msg.type == "human" else "ğŸ¤– AI"
                print(f"  {i}. {role}: {msg.content[:100]}...")
        else:
            print(f"âŒ ì„¸ì…˜ '{session_id}'ì˜ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def clear_chat_history(self, session_id: str = "default"):
        """íŠ¹ì • ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ ì‚­ì œ"""
        if session_id in self.store:
            del self.store[session_id]
            print(f"ğŸ—‘ï¸ ì„¸ì…˜ '{session_id}'ì˜ ëŒ€í™” ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"âŒ ì„¸ì…˜ '{session_id}'ì˜ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def save_vectorstore(self, save_path: str = "vectorstore"):
        """ë²¡í„°ìŠ¤í† ì–´ ì €ì¥"""
        if self.vectorstore:
            self.vectorstore.save_local(save_path)
            print(f"ğŸ’¾ ë²¡í„°ìŠ¤í† ì–´ ì €ì¥ë¨: {save_path}")
        else:
            print("âŒ ì €ì¥í•  ë²¡í„°ìŠ¤í† ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    def load_vectorstore(self, load_path: str = "vectorstore"):
        """ì €ì¥ëœ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ"""
        try:
            if not os.path.exists(load_path):
                print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {load_path}")
                return False
            
            embeddings = OpenAIEmbeddings(
                openai_api_key=self.openai_api_key,
                model="text-embedding-3-small"
            )
            
            self.vectorstore = FAISS.load_local(
                load_path, 
                embeddings,
                allow_dangerous_deserialization=True
            )
            
            # RAG ì²´ì¸ ì¬ìƒì„±
            self.rag_chain = self.create_rag_chain(self.vectorstore)
            self.conversational_rag_chain = self.create_conversational_rag_chain(self.rag_chain)
            
            print(f"âœ… ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œë¨: {load_path}")
            return True
        except Exception as e:
            print(f"âŒ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False