"""
ğŸš€ í˜„ì—… ë ˆë²¨ ìŠ¤íŠ¸ë¦¬ë° LLM ì„œë¹„ìŠ¤
ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì²´ê° ì†ë„ ê·¹ëŒ€í™”
"""
import asyncio
import json
import time
from typing import AsyncGenerator, Dict, Any, Optional
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from django.core.cache import cache
import logging
import os

logger = logging.getLogger(__name__)

class StreamingLLMService:
    """ğŸš€ í˜„ì—…ìš© ìŠ¤íŠ¸ë¦¬ë° LLM ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.streaming_llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.1,
            api_key=self.api_key,
            streaming=True,  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
            max_tokens=200,
            timeout=8,
            max_retries=1
        )
        
        # ìœ ì‚¬ ì§ˆë¬¸ ìºì‹œë¥¼ ìœ„í•œ ì„ë² ë”© ëª¨ë¸
        try:
            from langchain_openai import OpenAIEmbeddings
            self.embeddings = OpenAIEmbeddings(openai_api_key=self.api_key)
        except Exception as e:
            logger.warning(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.embeddings = None

    async def get_streaming_response(
        self, 
        phone_id: str, 
        user_text: str,
        use_cache: bool = True
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
        
        start_time = time.time()
        
        # 1. ì™„ì „ ì¼ì¹˜ ìºì‹œ í™•ì¸
        if use_cache:
            cache_key = f"llm_response:{phone_id}:{hash(user_text)}"
            cached_response = cache.get(cache_key)
            
            if cached_response:
                logger.info(f"âš¡ ìºì‹œ íˆíŠ¸: {time.time() - start_time:.3f}ì´ˆ")
                yield {
                    "type": "cached_response",
                    "content": cached_response,
                    "is_final": True,
                    "processing_time": time.time() - start_time,
                    "source": "cache"
                }
                return

        # 2. ìœ ì‚¬ ì§ˆë¬¸ ìºì‹œ í™•ì¸
        if use_cache and self.embeddings:
            similar_response = await self._check_similar_cache(phone_id, user_text)
            if similar_response:
                logger.info(f"âš¡ ìœ ì‚¬ ìºì‹œ íˆíŠ¸: {time.time() - start_time:.3f}ì´ˆ")
                yield {
                    "type": "similar_response",
                    "content": similar_response,
                    "is_final": True,
                    "processing_time": time.time() - start_time,
                    "source": "similar_cache"
                }
                return

        # 3. ë¹ ë¥¸ ì‹œì‘ ì‘ë‹µ
        yield {
            "type": "start",
            "content": "",
            "is_final": False,
            "processing_time": time.time() - start_time,
            "source": "streaming"
        }

        # 4. ìŠ¤íŠ¸ë¦¬ë° LLM ì‘ë‹µ
        try:
            messages = [
                SystemMessage(content="ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ê°„ê²°í•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”."),
                HumanMessage(content=f"[ì„¸ì…˜: {phone_id}] {user_text}")
            ]
            
            full_response = ""
            chunk_count = 0
            
            # ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            async for chunk in self.streaming_llm.astream(messages):
                chunk_count += 1
                
                if hasattr(chunk, 'content') and chunk.content:
                    full_response += chunk.content
                    
                    yield {
                        "type": "chunk",
                        "content": chunk.content,
                        "full_content": full_response,
                        "is_final": False,
                        "chunk_index": chunk_count,
                        "processing_time": time.time() - start_time,
                        "source": "streaming"
                    }

            # 5. ìµœì¢… ì‘ë‹µ ë° ìºì‹±
            total_time = time.time() - start_time
            
            if full_response:
                # ì‘ë‹µ ìºì‹± (5ë¶„)
                cache_key = f"llm_response:{phone_id}:{hash(user_text)}"
                cache.set(cache_key, full_response, timeout=300)
                
                # ìœ ì‚¬ ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ìºì‹±
                if self.embeddings:
                    await self._cache_for_similarity(phone_id, user_text, full_response)

            yield {
                "type": "final",
                "content": full_response,
                "is_final": True,
                "total_chunks": chunk_count,
                "processing_time": total_time,
                "source": "streaming"
            }
            
            logger.info(f"ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {chunk_count}ì²­í¬, {total_time:.3f}ì´ˆ")

        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            yield {
                "type": "error",
                "content": f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "is_final": True,
                "processing_time": time.time() - start_time,
                "source": "error"
            }

    async def _check_similar_cache(self, phone_id: str, user_text: str) -> Optional[str]:
        """ìœ ì‚¬ ì§ˆë¬¸ ìºì‹œ í™•ì¸"""
        try:
            # í˜„ì¬ ì§ˆë¬¸ì˜ ì„ë² ë”© ìƒì„±
            current_embedding = await asyncio.to_thread(
                self.embeddings.embed_query, user_text
            )
            
            # ìºì‹œëœ ì§ˆë¬¸ë“¤ê³¼ ìœ ì‚¬ë„ ë¹„êµ
            cache_pattern = f"llm_embedding:{phone_id}:*"
            # Redis íŒ¨í„´ ê²€ìƒ‰ (ì‹¤ì œ êµ¬í˜„ì‹œ Redis ì‚¬ìš©)
            
            # ê°„ë‹¨í•œ êµ¬í˜„: ìµœê·¼ 5ê°œ ì§ˆë¬¸ê³¼ ë¹„êµ
            for i in range(5):
                cached_key = f"llm_embedding:{phone_id}:{i}"
                cached_data = cache.get(cached_key)
                
                if cached_data:
                    similarity = self._cosine_similarity(
                        current_embedding, 
                        cached_data['embedding']
                    )
                    
                    # 85% ì´ìƒ ìœ ì‚¬í•˜ë©´ ìºì‹œëœ ì‘ë‹µ ì‚¬ìš©
                    if similarity > 0.85:
                        logger.info(f"âš¡ ìœ ì‚¬ ì§ˆë¬¸ ë°œê²¬: {similarity:.3f}")
                        return cached_data['response']
            
            return None
            
        except Exception as e:
            logger.warning(f"ìœ ì‚¬ ìºì‹œ í™•ì¸ ì‹¤íŒ¨: {e}")
            return None

    async def _cache_for_similarity(self, phone_id: str, user_text: str, response: str):
        """ìœ ì‚¬ ê²€ìƒ‰ì„ ìœ„í•œ ì„ë² ë”© ìºì‹±"""
        try:
            embedding = await asyncio.to_thread(
                self.embeddings.embed_query, user_text
            )
            
            # ìˆœí™˜ ìºì‹± (ìµœëŒ€ 5ê°œ)
            import random
            cache_index = random.randint(0, 4)
            cache_key = f"llm_embedding:{phone_id}:{cache_index}"
            
            cache.set(cache_key, {
                'text': user_text,
                'response': response,
                'embedding': embedding,
                'timestamp': time.time()
            }, timeout=1800)  # 30ë¶„
            
        except Exception as e:
            logger.warning(f"ìœ ì‚¬ ìºì‹± ì‹¤íŒ¨: {e}")

    def _cosine_similarity(self, vec1, vec2):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        import numpy as np
        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
streaming_llm_service = StreamingLLMService() 