"""
ğŸš€ Ultra Fast LLM System - í†µí•© ìµœì í™” ì‹œìŠ¤í…œ
ëª¨ë“  ìµœì í™” ê¸°ìˆ ì„ í†µí•©í•œ ì´ˆê³ ì† LLM ì‘ë‹µ ì‹œìŠ¤í…œ

ì„±ëŠ¥ ëª©í‘œ:
- 95% ìš”ì²­ì´ 0.2ì´ˆ ì´ë‚´ ì‘ë‹µ
- ìºì‹œ íˆíŠ¸ìœ¨ 80% ì´ìƒ
- 99.9% ê°€ìš©ì„±
"""
import asyncio
import time
import logging
from typing import Dict, Any, Optional, List
from ai.cache.multi_layer_cache import multi_cache
from ai.llm.parallel_llm import parallel_llm
from ai.llm.local_llm import local_llm_system
from ai.monitoring.performance_monitor import performance_monitor

logger = logging.getLogger(__name__)

class UltraFastLLMSystem:
    """ì´ˆê³ ì† LLM í†µí•© ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.is_initialized = False
        self.initialization_lock = asyncio.Lock()
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'llm_calls': 0,
            'avg_response_time': 0.0,
            'success_rate': 100.0
        }
        
        # ì‘ë‹µ ì „ëµ ìš°ì„ ìˆœìœ„
        self.response_strategies = [
            'memory_cache',        # 0.0001ì´ˆ - ë©”ëª¨ë¦¬ ìºì‹œ
            'redis_cache',         # 0.01ì´ˆ - Redis ìºì‹œ
            'semantic_cache',      # 0.1ì´ˆ - ì˜ë¯¸ì  ìœ ì‚¬ë„ ìºì‹œ  
            'local_llm_fast',      # 0.2ì´ˆ - ë¡œì»¬ LLM (ì´ˆê³ ì†)
            'parallel_llm_race',   # 0.5ì´ˆ - ë³‘ë ¬ LLM ë ˆì´ìŠ¤
            'fallback_response'    # 0.001ì´ˆ - í´ë°± ì‘ë‹µ
        ]
    
    async def initialize(self) -> bool:
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        async with self.initialization_lock:
            if self.is_initialized:
                return True
            
            logger.info("ğŸš€ Ultra Fast LLM System ì´ˆê¸°í™” ì‹œì‘...")
            
            try:
                # 1. ë¡œì»¬ LLM ì‹œìŠ¤í…œ ì´ˆê¸°í™”
                await local_llm_system.initialize()
                logger.info("âœ… ë¡œì»¬ LLM ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
                
                # 2. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
                await performance_monitor.start_monitoring()
                logger.info("âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
                
                # 3. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
                health_status = await self.health_check()
                logger.info(f"ğŸ¥ ì‹œìŠ¤í…œ ìƒíƒœ: {health_status}")
                
                self.is_initialized = True
                logger.info("ğŸ‰ Ultra Fast LLM System ì´ˆê¸°í™” ì™„ë£Œ!")
                
                return True
                
            except Exception as e:
                logger.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                return False
    
    async def generate_response(self, query: str, phone_id: str, **kwargs) -> Dict[str, Any]:
        """ì´ˆê³ ì† ì‘ë‹µ ìƒì„± - ëª¨ë“  ìµœì í™” ê¸°ë²• ì ìš©"""
        
        if not self.is_initialized:
            await self.initialize()
        
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        try:
            # ğŸš€ Strategy 1-3: Multi-Layer Cache (0.0001ì´ˆ ~ 0.1ì´ˆ)
            cache_result = await multi_cache.get(query, phone_id)
            if cache_result:
                response_time = time.time() - start_time
                self.stats['cache_hits'] += 1
                await self._record_success(query, cache_result['content'], phone_id,
                                         cache_result['source'], response_time, cache_hit=True)
                return {
                    **cache_result,
                    'cache_hit': True,
                    'strategy_used': 'multi_layer_cache'
                }
            
            # ğŸš€ Strategy 4: ë¡œì»¬ LLM ì´ˆê³ ì† (0.2ì´ˆ)
            local_result = await self._try_local_llm_fast(query, phone_id)
            if local_result and local_result['success']:
                response_time = time.time() - start_time
                # ìºì‹œì— ì €ì¥
                await multi_cache.set(query, local_result['content'], phone_id)
                await self._record_success(query, local_result['content'], phone_id,
                                         'local_llm', response_time)
                return {
                    **local_result,
                    'strategy_used': 'local_llm_fast'
                }
            
            # ğŸš€ Strategy 5: ë³‘ë ¬ LLM ë ˆì´ìŠ¤ (0.5ì´ˆ)
            parallel_result = await self._try_parallel_llm(query, phone_id)
            if parallel_result and parallel_result['success']:
                response_time = time.time() - start_time
                # ìºì‹œì— ì €ì¥
                await multi_cache.set(query, parallel_result['content'], phone_id)
                await self._record_success(query, parallel_result['content'], phone_id,
                                         'parallel_llm', response_time)
                return {
                    **parallel_result,
                    'strategy_used': 'parallel_llm_race'
                }
            
            # ğŸš€ Strategy 6: í´ë°± ì‘ë‹µ (0.001ì´ˆ)
            fallback_response = self._get_fallback_response(query)
            response_time = time.time() - start_time
            await self._record_success(query, fallback_response, phone_id,
                                     'fallback', response_time, is_fallback=True)
            return {
                'content': fallback_response,
                'source': 'fallback',
                'processing_time': response_time,
                'is_fallback': True,
                'strategy_used': 'fallback_response'
            }
            
        except Exception as e:
            response_time = time.time() - start_time
            logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {e}")
            
            # ì—ëŸ¬ ê¸°ë¡
            await self._record_error(query, phone_id, str(e), response_time)
            
            # ì—ëŸ¬ ì‹œì—ë„ í´ë°± ì‘ë‹µ
            return {
                'content': "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
                'source': 'error_fallback',
                'processing_time': response_time,
                'error': str(e),
                'success': False
            }
    
    
    async def _try_local_llm_fast(self, query: str, phone_id: str) -> Optional[Dict[str, Any]]:
        """ë¡œì»¬ LLM ì´ˆê³ ì† ì‹œë„"""
        try:
            # íƒ€ì„ì•„ì›ƒì„ ì§§ê²Œ ì„¤ì •í•˜ì—¬ ì´ˆê³ ì† ëª¨ë“œ
            result = await asyncio.wait_for(
                local_llm_system.generate_fast(query, timeout=0.5, max_tokens=50),
                timeout=1.0
            )
            return result
        except Exception as e:
            logger.warning(f"ë¡œì»¬ LLM ì‹¤íŒ¨: {e}")
            return None
    
    async def _try_parallel_llm(self, query: str, phone_id: str) -> Optional[Dict[str, Any]]:
        """ë³‘ë ¬ LLM ì‹œë„"""
        try:
            messages = [{"role": "user", "content": query}]
            result = await parallel_llm.generate_parallel(
                messages, 
                strategy="race_with_timeout",
                quality_check=True
            )
            return result
        except Exception as e:
            logger.warning(f"ë³‘ë ¬ LLM ì‹¤íŒ¨: {e}")
            return None
    
    def _get_fallback_response(self, query: str) -> str:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        # í•­ìƒ LLM í˜¸ì¶œì„ ìœ ë„í•˜ëŠ” ê¸°ë³¸ ì‘ë‹µ
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
    
    async def _record_success(self, query: str, response: str, phone_id: str,
                            source: str, processing_time: float, 
                            cache_hit: bool = False, is_fallback: bool = False) -> None:
        """ì„±ê³µ ê¸°ë¡"""
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ê¸°ë¡
        performance_monitor.record_request(
            source=source,
            processing_time=processing_time,
            success=True,
            cache_hit=cache_hit,
            model_used=source,
            is_fallback=is_fallback,
            query_length=len(query),
            response_length=len(response)
        )
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self.stats['avg_response_time'] = (
            self.stats['avg_response_time'] * (self.stats['total_requests'] - 1) + processing_time
        ) / self.stats['total_requests']
        
        logger.info(f"âœ… {source} ì„±ê³µ: {processing_time:.3f}ì´ˆ - {response[:50]}...")
    
    async def _record_error(self, query: str, phone_id: str, error: str, processing_time: float) -> None:
        """ì—ëŸ¬ ê¸°ë¡"""
        performance_monitor.record_request(
            source='error',
            processing_time=processing_time,
            success=False,
            cache_hit=False,
            error=error,
            query_length=len(query)
        )
        
        # ì„±ê³µë¥  ì—…ë°ì´íŠ¸
        successful_requests = self.stats['total_requests'] - 1  # í˜„ì¬ ì‹¤íŒ¨ ì œì™¸
        self.stats['success_rate'] = (successful_requests / self.stats['total_requests']) * 100
        
        logger.error(f"âŒ ì—ëŸ¬ ê¸°ë¡: {error}")
    
    async def health_check(self) -> Dict[str, Any]:
        """ì „ì²´ ì‹œìŠ¤í…œ í—¬ìŠ¤ ì²´í¬"""
        health_status = {
            'overall_status': 'healthy',
            'timestamp': time.time(),
            'components': {}
        }
        
        try:
            # 1. ìºì‹œ ì‹œìŠ¤í…œ ì²´í¬
            cache_stats = multi_cache.get_cache_stats()
            health_status['components']['cache'] = {
                'status': 'healthy',
                'stats': cache_stats
            }
            
            # 2. ë³‘ë ¬ LLM ì‹œìŠ¤í…œ ì²´í¬
            parallel_health = await parallel_llm.health_check()
            healthy_providers = sum(1 for status in parallel_health.values() 
                                 if status.get('status') == 'healthy')
            health_status['components']['parallel_llm'] = {
                'status': 'healthy' if healthy_providers > 0 else 'degraded',
                'healthy_providers': healthy_providers,
                'total_providers': len(parallel_health),
                'details': parallel_health
            }
            
            # 3. ë¡œì»¬ LLM ì‹œìŠ¤í…œ ì²´í¬
            local_health = await local_llm_system.health_check()
            health_status['components']['local_llm'] = {
                'status': 'healthy' if local_health.get('ollama_status') else 'unavailable',
                'details': local_health
            }
            
            # 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì²´í¬
            dashboard_data = performance_monitor.get_dashboard_data()
            if dashboard_data:
                snapshot = dashboard_data.get('snapshot', {})
                health_status['components']['performance'] = {
                    'status': 'healthy',
                    'response_time_avg': snapshot.get('response_time_avg', 0),
                    'cache_hit_rate': snapshot.get('cache_hit_rate', 0),
                    'error_rate': snapshot.get('error_rate', 0)
                }
            
            # ì „ì²´ ìƒíƒœ ê²°ì •
            component_statuses = [comp['status'] for comp in health_status['components'].values()]
            if 'unhealthy' in component_statuses:
                health_status['overall_status'] = 'unhealthy'
            elif 'degraded' in component_statuses or 'unavailable' in component_statuses:
                health_status['overall_status'] = 'degraded'
            
        except Exception as e:
            health_status['overall_status'] = 'error'
            health_status['error'] = str(e)
        
        return health_status
    
    def get_system_stats(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ í†µê³„"""
        # ìºì‹œ í†µê³„
        cache_stats = multi_cache.get_cache_stats()
        
        # ë³‘ë ¬ LLM í†µê³„
        parallel_stats = parallel_llm.get_stats()
        
        # ë¡œì»¬ LLM í†µê³„
        local_stats = local_llm_system.get_system_stats()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì´í„°
        performance_data = performance_monitor.get_dashboard_data()
        
        return {
            'ultra_fast_llm': self.stats,
            'cache_system': cache_stats,
            'parallel_llm': parallel_stats,
            'local_llm': local_stats,
            'performance_monitoring': performance_data,
            'system_status': 'initialized' if self.is_initialized else 'not_initialized'
        }
    
    async def optimize_system(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìë™ ìµœì í™”"""
        optimization_results = {
            'optimizations_applied': [],
            'performance_improvement': {},
            'timestamp': time.time()
        }
        
        try:
            # ì„±ëŠ¥ ë°ì´í„° ë¶„ì„
            performance_data = performance_monitor.get_dashboard_data()
            if not performance_data:
                return optimization_results
            
            snapshot = performance_data.get('snapshot', {})
            suggestions = performance_data.get('suggestions', [])
            
            for suggestion in suggestions:
                if suggestion['priority'] == 'high':
                    suggestion_type = suggestion['type']
                    
                    if suggestion_type == 'cache_optimization':
                        # ìºì‹œ TTL ì¦ê°€
                        logger.info("ğŸ”§ ìºì‹œ ìµœì í™” ì ìš©")
                        optimization_results['optimizations_applied'].append('cache_ttl_increased')
                    
                    elif suggestion_type == 'parallel_optimization':
                        # ë³‘ë ¬ ëª¨ë¸ íƒ€ì„ì•„ì›ƒ ê°ì†Œ
                        logger.info("ğŸ”§ ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™” ì ìš©")
                        optimization_results['optimizations_applied'].append('parallel_timeout_reduced')
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            multi_cache.memory_cache.clear_expired()
            optimization_results['optimizations_applied'].append('memory_cleanup')
            
            logger.info(f"ğŸš€ ì‹œìŠ¤í…œ ìµœì í™” ì™„ë£Œ: {len(optimization_results['optimizations_applied'])}ê°œ í•­ëª©")
            
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ìµœì í™” ì˜¤ë¥˜: {e}")
            optimization_results['error'] = str(e)
        
        return optimization_results

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
ultra_fast_llm = UltraFastLLMSystem()