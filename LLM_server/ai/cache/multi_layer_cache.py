"""
ğŸš€ Multi-Layer Cache System
L0: Memory Cache (0.0001ì´ˆ)
L1: Redis Exact Cache (0.01ì´ˆ)  
L2: Semantic Similarity Cache (0.1ì´ˆ)
L3: Predictive Cache (ë°±ê·¸ë¼ìš´ë“œ)
"""
import asyncio
import hashlib
import json
import time
import logging
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
from django.core.cache import cache
import threading
from collections import OrderedDict
import numpy as np

logger = logging.getLogger(__name__)

class MemoryCache:
    """L0: ì´ˆê³ ì† ë©”ëª¨ë¦¬ ìºì‹œ"""
    
    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self.cache = OrderedDict()
        self.access_times = {}
        self.lock = threading.RLock()
    
    def _is_expired(self, key: str) -> bool:
        if key not in self.access_times:
            return True
        return time.time() - self.access_times[key] > self.ttl_seconds
    
    def get(self, key: str) -> Optional[str]:
        with self.lock:
            if key in self.cache and not self._is_expired(key):
                # LRU: ìµœê·¼ ì‚¬ìš©ìœ¼ë¡œ ì´ë™
                self.cache.move_to_end(key)
                self.access_times[key] = time.time()
                return self.cache[key]
            elif key in self.cache:
                # ë§Œë£Œëœ í‚¤ ì œê±°
                del self.cache[key]
                del self.access_times[key]
        return None
    
    def set(self, key: str, value: str) -> None:
        with self.lock:
            # ìµœëŒ€ í¬ê¸° ì´ˆê³¼ì‹œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            while len(self.cache) >= self.max_size:
                oldest_key, _ = self.cache.popitem(last=False)
                self.access_times.pop(oldest_key, None)
            
            self.cache[key] = value
            self.access_times[key] = time.time()
            self.cache.move_to_end(key)
    
    def clear_expired(self) -> None:
        """ë§Œë£Œëœ í•­ëª©ë“¤ ì •ë¦¬"""
        with self.lock:
            expired_keys = [k for k in self.cache.keys() if self._is_expired(k)]
            for key in expired_keys:
                del self.cache[key]
                del self.access_times[key]
    
    def stats(self) -> Dict[str, Any]:
        with self.lock:
            return {
                'size': len(self.cache),
                'max_size': self.max_size,
                'utilization': len(self.cache) / self.max_size * 100
            }

class SemanticCache:
    """L2: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ìºì‹œ"""
    
    def __init__(self, similarity_threshold: float = 0.95):
        self.similarity_threshold = similarity_threshold
        self.embedding_cache = {}
        self.semantic_index = {}  # {embedding_hash: (query, response, embedding)}
        self.lock = threading.RLock()
    
    def _get_simple_embedding(self, text: str) -> np.ndarray:
        """ê°„ë‹¨í•œ TF-IDF ê¸°ë°˜ ì„ë² ë”© (ì‹¤ì œë¡  SentenceTransformer ì‚¬ìš© ê¶Œì¥)"""
        # í•œêµ­ì–´ ë¶ˆìš©ì–´
        stop_words = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì—', 'ì—ì„œ', 'ì™€', 'ê³¼', 'ì˜', 'ë¡œ', 'ìœ¼ë¡œ'}
        
        # í† í°í™” ë° ë¶ˆìš©ì–´ ì œê±°
        tokens = [word for word in text.lower().split() if word not in stop_words and len(word) > 1]
        
        # ê°„ë‹¨í•œ í•´ì‹œ ê¸°ë°˜ ë²¡í„° ìƒì„± (ì‹¤ì œë¡  ë” ì •êµí•œ ë°©ë²• í•„ìš”)
        vector = np.zeros(100)
        for i, token in enumerate(tokens[:10]):  # ìµœëŒ€ 10ê°œ í† í°
            hash_val = hash(token) % 100
            vector[hash_val] += 1.0 / (i + 1)  # ìœ„ì¹˜ ê°€ì¤‘ì¹˜
        
        # ì •ê·œí™”
        norm = np.linalg.norm(vector)
        return vector / norm if norm > 0 else vector
    
    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
        except:
            return 0.0
    
    async def find_similar(self, query: str, phone_id: str) -> Optional[str]:
        """ìœ ì‚¬í•œ ì§ˆë¬¸ì˜ ë‹µë³€ ì°¾ê¸°"""
        query_embedding = self._get_simple_embedding(query)
        
        with self.lock:
            best_similarity = 0.0
            best_response = None
            
            for key, (cached_query, cached_response, cached_embedding) in self.semantic_index.items():
                if not key.startswith(f"sem:{phone_id}:"):
                    continue
                    
                similarity = self._cosine_similarity(query_embedding, cached_embedding)
                
                if similarity > best_similarity and similarity >= self.similarity_threshold:
                    best_similarity = similarity
                    best_response = cached_response
            
            if best_response:
                logger.info(f"ğŸ¯ ì‹œë§¨í‹± ìºì‹œ íˆíŠ¸: ìœ ì‚¬ë„ {best_similarity:.3f}")
                return best_response
        
        return None
    
    async def store(self, query: str, response: str, phone_id: str) -> None:
        """ì‹œë§¨í‹± ìºì‹œì— ì €ì¥"""
        query_embedding = self._get_simple_embedding(query)
        key = f"sem:{phone_id}:{hashlib.md5(query.encode()).hexdigest()[:8]}"
        
        with self.lock:
            self.semantic_index[key] = (query, response, query_embedding)
            
            # ìµœëŒ€ í¬ê¸° ì œí•œ (ì‚¬ìš©ìë‹¹ 50ê°œ)
            user_keys = [k for k in self.semantic_index.keys() if k.startswith(f"sem:{phone_id}:")]
            if len(user_keys) > 50:
                # ê°€ì¥ ì˜¤ë˜ëœ ê²ƒ ì œê±°
                oldest_key = min(user_keys)
                del self.semantic_index[oldest_key]

class PredictiveCache:
    """L3: ì˜ˆì¸¡ ìºì‹œ - ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ ë¯¸ë¦¬ ìƒì„±"""
    
    def __init__(self):
        self.question_patterns = {}
        self.user_patterns = {}
        self.time_patterns = {}
        self.lock = threading.RLock()
    
    def record_question(self, phone_id: str, question: str, response: str) -> None:
        """ì§ˆë¬¸ íŒ¨í„´ ê¸°ë¡"""
        with self.lock:
            # ì „ì—­ íŒ¨í„´
            question_key = hashlib.md5(question.lower().encode()).hexdigest()[:8]
            if question_key not in self.question_patterns:
                self.question_patterns[question_key] = {'count': 0, 'response': response}
            self.question_patterns[question_key]['count'] += 1
            
            # ì‚¬ìš©ìë³„ íŒ¨í„´
            if phone_id not in self.user_patterns:
                self.user_patterns[phone_id] = {}
            if question_key not in self.user_patterns[phone_id]:
                self.user_patterns[phone_id][question_key] = {'count': 0, 'question': question, 'response': response}
            self.user_patterns[phone_id][question_key]['count'] += 1
            
            # ì‹œê°„ëŒ€ë³„ íŒ¨í„´
            hour = datetime.now().hour
            if hour not in self.time_patterns:
                self.time_patterns[hour] = {}
            if question_key not in self.time_patterns[hour]:
                self.time_patterns[hour][question_key] = {'count': 0, 'question': question, 'response': response}
            self.time_patterns[hour][question_key]['count'] += 1
    
    def get_popular_questions(self, phone_id: str, limit: int = 10) -> List[Tuple[str, str]]:
        """ì¸ê¸° ì§ˆë¬¸ ëª©ë¡"""
        with self.lock:
            # ì‚¬ìš©ìë³„ + ì „ì—­ ì¸ê¸° ì§ˆë¬¸
            all_patterns = []
            
            # ì‚¬ìš©ì íŒ¨í„´
            if phone_id in self.user_patterns:
                for pattern in self.user_patterns[phone_id].values():
                    all_patterns.append((pattern['question'], pattern['response'], pattern['count'] * 2))  # ì‚¬ìš©ì íŒ¨í„´ ê°€ì¤‘ì¹˜
            
            # ì „ì—­ íŒ¨í„´  
            for pattern in self.question_patterns.values():
                all_patterns.append(("", pattern['response'], pattern['count']))
            
            # ë¹ˆë„ìˆœ ì •ë ¬
            all_patterns.sort(key=lambda x: x[2], reverse=True)
            
            return [(q, r) for q, r, _ in all_patterns[:limit] if q]

class MultiLayerCache:
    """í†µí•© ë‹¤ì¸µ ìºì‹œ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.memory_cache = MemoryCache()
        self.semantic_cache = SemanticCache()
        self.predictive_cache = PredictiveCache()
        
        # ì„±ëŠ¥ í†µê³„
        self.stats = {
            'l0_hits': 0,  # Memory
            'l1_hits': 0,  # Redis Exact
            'l2_hits': 0,  # Semantic
            'l3_hits': 0,  # Predictive
            'misses': 0,
            'total_requests': 0
        }
        
        # ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì‘ì—… ì‹œì‘
        self._start_cleanup_tasks()
    
    def _generate_cache_key(self, query: str, phone_id: str, cache_type: str = "exact") -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        query_hash = hashlib.md5(query.encode()).hexdigest()
        return f"{cache_type}:{phone_id}:{query_hash}"
    
    async def get(self, query: str, phone_id: str) -> Optional[Dict[str, Any]]:
        """ë‹¤ì¸µ ìºì‹œì—ì„œ ì‘ë‹µ ì¡°íšŒ"""
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        # L0: Memory Cache (0.0001ì´ˆ)
        memory_key = self._generate_cache_key(query, phone_id, "mem")
        result = self.memory_cache.get(memory_key)
        if result:
            self.stats['l0_hits'] += 1
            logger.info(f"âš¡ L0 ë©”ëª¨ë¦¬ ìºì‹œ íˆíŠ¸: {(time.time() - start_time) * 1000:.2f}ms")
            return {
                'content': result,
                'source': 'L0_memory',
                'processing_time': time.time() - start_time
            }
        
        # L1: Redis Exact Cache (0.01ì´ˆ)
        redis_key = self._generate_cache_key(query, phone_id, "exact")
        result = cache.get(redis_key)
        if result:
            self.stats['l1_hits'] += 1
            # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
            self.memory_cache.set(memory_key, result)
            logger.info(f"ğŸ’¾ L1 Redis ìºì‹œ íˆíŠ¸: {(time.time() - start_time) * 1000:.2f}ms")
            return {
                'content': result,
                'source': 'L1_redis',
                'processing_time': time.time() - start_time
            }
        
        # L2: Semantic Similarity Cache (0.1ì´ˆ)
        result = await self.semantic_cache.find_similar(query, phone_id)
        if result:
            self.stats['l2_hits'] += 1
            # ìƒìœ„ ìºì‹œì—ë„ ì €ì¥
            self.memory_cache.set(memory_key, result)
            cache.set(redis_key, result, timeout=1800)
            logger.info(f"ğŸ¯ L2 ì‹œë§¨í‹± ìºì‹œ íˆíŠ¸: {(time.time() - start_time) * 1000:.2f}ms")
            return {
                'content': result,
                'source': 'L2_semantic',
                'processing_time': time.time() - start_time
            }
        
        # L3: Predictive Cache (ì „ì—­ ì¸ê¸° ë‹µë³€)
        popular_questions = self.predictive_cache.get_popular_questions(phone_id, 5)
        for pop_question, pop_response in popular_questions:
            if self._is_similar_simple(query, pop_question):
                self.stats['l3_hits'] += 1
                # ëª¨ë“  ìƒìœ„ ìºì‹œì— ì €ì¥
                await self.set(query, pop_response, phone_id)
                logger.info(f"ğŸ”® L3 ì˜ˆì¸¡ ìºì‹œ íˆíŠ¸: {(time.time() - start_time) * 1000:.2f}ms")
                return {
                    'content': pop_response,
                    'source': 'L3_predictive',
                    'processing_time': time.time() - start_time
                }
        
        # ìºì‹œ ë¯¸ìŠ¤
        self.stats['misses'] += 1
        logger.info(f"âŒ ëª¨ë“  ìºì‹œ ë¯¸ìŠ¤: {(time.time() - start_time) * 1000:.2f}ms")
        return None
    
    async def set(self, query: str, response: str, phone_id: str) -> None:
        """ëª¨ë“  ìºì‹œ ë ˆì´ì–´ì— ì €ì¥"""
        # L0: Memory
        memory_key = self._generate_cache_key(query, phone_id, "mem")
        self.memory_cache.set(memory_key, response)
        
        # L1: Redis
        redis_key = self._generate_cache_key(query, phone_id, "exact")
        cache.set(redis_key, response, timeout=1800)  # 30ë¶„
        
        # L2: Semantic
        await self.semantic_cache.store(query, response, phone_id)
        
        # L3: Predictive (íŒ¨í„´ ê¸°ë¡)
        self.predictive_cache.record_question(phone_id, query, response)
        
        logger.info(f"ğŸ’¾ ëª¨ë“  ë ˆì´ì–´ì— ìºì‹± ì™„ë£Œ: {query[:30]}...")
    
    def _is_similar_simple(self, query1: str, query2: str) -> bool:
        """ê°„ë‹¨í•œ ìœ ì‚¬ë„ ì²´í¬"""
        if not query2:
            return False
        
        words1 = set(query1.lower().split())
        words2 = set(query2.lower().split())
        
        if not words1 or not words2:
            return False
        
        intersection = len(words1 & words2)
        union = len(words1 | words2)
        
        return intersection / union > 0.6 if union > 0 else False
    
    def _start_cleanup_tasks(self) -> None:
        """ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì‘ì—… ì‹œì‘"""
        def cleanup_worker():
            while True:
                try:
                    # 5ë¶„ë§ˆë‹¤ ì •ë¦¬ ì‘ì—…
                    time.sleep(300)
                    self.memory_cache.clear_expired()
                    logger.info("ğŸ§¹ ìºì‹œ ì •ë¦¬ ì‘ì—… ì™„ë£Œ")
                except Exception as e:
                    logger.error(f"ìºì‹œ ì •ë¦¬ ì˜¤ë¥˜: {e}")
        
        cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        cleanup_thread.start()
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ ì„±ëŠ¥ í†µê³„"""
        total = self.stats['total_requests']
        if total == 0:
            return self.stats
        
        return {
            **self.stats,
            'l0_hit_rate': self.stats['l0_hits'] / total * 100,
            'l1_hit_rate': self.stats['l1_hits'] / total * 100,
            'l2_hit_rate': self.stats['l2_hits'] / total * 100,
            'l3_hit_rate': self.stats['l3_hits'] / total * 100,
            'total_hit_rate': (total - self.stats['misses']) / total * 100,
            'memory_stats': self.memory_cache.stats()
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
multi_cache = MultiLayerCache()